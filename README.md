*    Just a base tool for automatically logging into Linkedin, searching by keyword in the scrape_jobs.py first driver.get() method, and scraping job listings with a random timer delay that will stop once there the search results runs a limit, which is 10 job listings per page. 
*    Scraped jobs are appended to the results4.csv file, as indicated in _parameters.py, which also stores linkedin login information.
*    From a Jupyter notebook, one can then sort the data using dataframe manipuation to extract by more specifics such as keywords in the job description, or sort by jobs posted hours ago. 

### Why this, why now?
Automation provides us with wondrous increases of production and information, but does it tell us what to do with the men the machines displace? Modern industry gives us the capacity for unparalleled wealth - but where is our capacity to make that wealth meaningful to the poor of every nation? -Robert Kennedy

Our human worth in regards to our ability to contribute to society is now determined by Applicant Tracking Software, why not write an algorithmn to converse with employers at scale. Just casting a wider net here. It's just a project, it's just my time.
